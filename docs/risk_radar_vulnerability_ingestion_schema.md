
Vulnerability Ingestion Schema and Field Mapping

Integrating multiple vulnerability scanners into a unified platform requires a flexible data model. Each scanner (Tenable, Nessus, Qualys, CrowdStrike, Microsoft Defender VM, etc.) may use different identifiers and severity scales, which can lead to duplicate records and inconsistent data if not normalized ￼ ￼. The following schema defines core tables and mapping logic to ingest and normalize assets and vulnerabilities across integrations, with emphasis on extensibility, deduplication, and normalization.

Schema Overview

The schema centers on separate tables for assets, vulnerabilities, and findings to normalize data and avoid duplication. A scanner_integration table lists all configured integrations (connectors), while field_mapping and severity_mapping tables provide flexible mapping from vendor-specific fields and severity ratings to the internal schema. Foreign keys link related data, and JSONB columns capture unstructured or vendor-specific metadata for extensibility. This design ensures that data from many scanners can be consolidated and correlated (e.g. correlating Tenable “plugin” findings with CrowdStrike CVE findings) in a single repository

Core Table Schemas

Below are SQL definitions for each core table, followed by explanations of their fields, constraints, and how they accommodate various vendor data.

scanner_integration Table
```sql
CREATE TABLE scanner_integration (
    integration_id SERIAL PRIMARY KEY,
    name VARCHAR(100) NOT NULL UNIQUE,
    type VARCHAR(50) NOT NULL,               -- e.g., 'vuln_scanner', 'asset_inventory'
    description TEXT,
    active BOOLEAN DEFAULT TRUE
);
```

Description: Lists each external scanner integration (connector) configured in the system. For example, entries might include “Nessus File”, “Tenable.io”, “Qualys VM”, “CrowdStrike Falcon Spotlight”, or “Microsoft Defender VM”. The type field can categorize the integration (e.g. all the above are vulnerability scanners). This table is referenced by other tables to scope field mappings and severity mappings to a specific integration.
	•	name: Human-readable name of the integration (must be unique, e.g. “Tenable.io” or “Qualys”).
	•	type: Category of integration (for extensibility – e.g., some connectors might be asset-only or ticketing systems, though here primarily 'vuln_scanner').
	•	description: Optional text describing the integration or version.
	•	active: Whether the integration is currently in use (for enabling/disabling data sync).

Usage & Extensibility: Adding support for a new scanner involves inserting a new row here. This table’s primary key (integration_id) is used as a foreign key in other tables to associate assets, findings, and mapping rules with the source integration.

assets Table
```sql
CREATE TABLE assets (
    asset_id SERIAL PRIMARY KEY,
    hostname VARCHAR(255),
    ip_address INET,
    asset_type VARCHAR(50),         -- e.g., 'Server', 'Workstation', 'Container'
    operating_system VARCHAR(100),
    mac_address VARCHAR(50),
    extra JSONB,                    -- unstructured metadata (e.g., cloud IDs, tags)
    UNIQUE(hostname, ip_address)
);
```

Description: Stores unique IT assets (hosts, devices, etc.) discovered by scanners. Each asset record may aggregate data from multiple integrations (to avoid duplicate asset entries for the same host). The schema includes common identity fields and an extra JSONB column for any scanner-specific metadata that doesn’t have a dedicated column.
	•	hostname / ip_address: Core identifiers for the asset. These may come from scanner data (e.g., Nessus HostName or NetBIOS name, Qualys DNS hostname, etc.). A unique constraint on (hostname, ip_address) helps prevent exact duplicates, though comprehensive deduplication may require more logic (different tools may label the same asset differently ￼). Both fields are nullable to accommodate assets identified by IP only or name only, but at least one should be present.
	•	asset_type: High-level classification of the asset (if provided by the scanner or set manually). For instance, integrations like Microsoft Intune might classify device type; vulnerability scanners might not always provide this, but the field is available for normalization if needed.
	•	operating_system: OS name or fingerprint (e.g., “Windows 10”, “Ubuntu 20.04”) as reported by the scanner. Different scanners provide OS info in various forms (Qualys might give a banner, Nessus has an operating-system field in host properties), so this field stores a normalized version if available.
	•	mac_address: MAC address of the asset, if applicable (commonly provided for network devices or if scanner has agent data).
	•	extra (JSONB): Flexible JSON storage for additional metadata. This can include cloud-specific identifiers (e.g., AWS instance ID, Azure VM ID), asset tags, or any other fields from specific integrations. For example, CrowdStrike might provide a host device ID or policy, which can be stored here; Qualys might provide an asset UUID or vulnerability count, etc. Storing such data in JSONB allows new info to be captured without altering the schema for each vendor’s unique fields.

Extensibility & Deduplication: The asset schema is designed to consolidate data from multiple sources. The unique index on hostname+ip_address provides a basic deduplication key (preventing exact duplicates); however, true deduplication in practice may require more sophisticated checks (e.g., alias IPs, differing host naming conventions, etc. ￼). If two sources use different identifiers (one by IP, another by hostname), correlation logic is needed to merge them into one assets record. This schema supports merging by updating a single asset record with data from multiple sources (e.g., populating missing hostname or IP and aggregating metadata in extra). Foreign keys in the findings table (detailed below) link each finding to a single unified asset record, regardless of which integration reported it.

vulnerabilities Table
```sql
CREATE TABLE vulnerabilities (
    vulnerability_id SERIAL PRIMARY KEY,
    cve_id VARCHAR(50),                     -- e.g., 'CVE-2023-12345'
    external_id VARCHAR(100),               -- scanner-specific vuln ID (e.g., plugin ID, QID)
    external_source VARCHAR(50),            -- name of source of external_id (e.g., 'Nessus', 'Qualys')
    title TEXT NOT NULL,
    description TEXT,
    cvss_score NUMERIC(4,1),                -- e.g., CVSS base score 0.0 - 10.0
    severity_level SMALLINT,                -- normalized severity (e.g., 0-10 scale)
    severity_label VARCHAR(20),             -- normalized severity label (e.g., 'Critical','High')
    fix_info TEXT,                          -- remediation summary or patch info
    extra JSONB,
    UNIQUE(cve_id),
    UNIQUE(external_source, external_id)
);
```

Description: Stores unique vulnerabilities or security issues in a normalized way. This table abstracts a vulnerability (e.g., a CVE or a scanner-specific issue) independent of any particular asset. By separating vulnerabilities, we avoid duplicating descriptive info across multiple findings. Fields are provided for both standard identifiers (like CVEs) and scanner-specific identifiers, to support deduplication and correlation of findings from different sources ￼.
	•	cve_id: Standard CVE identifier if the vulnerability is a known CVE (e.g., “CVE-2023-12345”). This allows grouping all scanner findings referencing the same CVE under one record. The schema enforces uniqueness on cve_id so the same CVE is not entered twice. (If multiple CVEs apply to essentially one finding, those can be listed in the description or extra since typically one primary CVE is used per vulnerability entry.)
	•	external_id & external_source: For vulnerabilities that may not have a CVE or have vendor-specific identifiers, these fields capture the scanner’s identifier and its source. For example, Nessus plugin ID “10000” with source “Nessus”, Qualys QID “13044” with source “Qualys”, or Microsoft Defender vulnerability ID with source “Defender”. The combination (external_source, external_id) is unique, ensuring no duplicate records for the same scanner-specific vulnerability. These fields allow correlation when a vulnerability lacks a CVE – e.g., if Tenable Nessus and Qualys both report a custom issue with no CVE, we could map both to one vulnerabilities record by matching some criteria and storing one of the IDs (others could be noted in extra or a separate mapping table if needed). In cases where a scanner provides both a CVE and its own ID, cve_id will be filled (for deduplication across sources) and external_id/source can also be stored to reference the scanner’s info (the unique constraints ensure another record with the same plugin/QID won’t be created).
	•	title: A short name for the vulnerability. Typically this might be the CVE title or plugin title (e.g., “OpenSSL Heartbleed Vulnerability”). If multiple sources use different titles for the same issue, the first source or CVE name can be used, or whichever is most descriptive, to avoid duplicates.
	•	description: A detailed description of the vulnerability, its impact, etc. This can be pulled from scanner data (e.g., Nessus plugin description, Qualys vulnerability description) or augmented from external databases (like NVD for CVE details).
	•	cvss_score: The CVSS base score (if available). Many scanners provide CVSSv2 or CVSSv3 scores for findings. This field allows storing a numeric severity according to the industry standard 0.0–10.0 scale. For example, Tenable.io and Qualys typically provide CVSS scores for CVE findings, which can be ingested directly. If a finding doesn’t come with a CVSS score, this might be left NULL or populated through a lookup (if we later enrich CVEs).
	•	severity_level & severity_label: The normalized severity rating in internal terms. This is typically derived via the severity_mapping table during ingestion. For instance, regardless of whether a scanner calls an issue “High” or gives it a score of 7.5, we might normalize to severity_label='High' with an internal severity_level (e.g. 8 on a 0–10 risk scale). Storing both a numeric level and a label allows flexible filtering and sorting. The severity_level could be mapped directly from CVSS score if available (or set via mapping rules). For example, a CVSS 7.5 might equate to internal 8 (“High”). If a scanner like CrowdStrike reports severity only qualitatively (e.g., “Critical”), our mapping can assign it a numeric level.
	•	fix_info: Remediation or fix information. Scanners often include a solution or recommended fix text (e.g., Nessus plugin “solution” section, Qualys remediation text). This field captures that to inform remediation efforts.
	•	extra (JSONB): Any additional vendor-specific data or metadata not covered by the standard fields. This could include references (links to external advisories), exploit availability, or scanner-specific flags. For example, Nessus provides plugin output for evidence, which we might store at the finding level (since it’s asset-specific), but any general references (like CWE IDs, or “see also” links from Nessus plugins) could be stored here as an array. Qualys provides categories like “PCI flagged” or “Bugtraq IDs” – those can be stored in JSON. Microsoft Defender might include a vulnerability ID or GUID which can be stored if not used as primary external_id.

Deduplication & Normalization: By using CVE IDs and unique scanner IDs, this table helps eliminate duplicate vulnerability definitions. If two different integrations report the same CVE, they will map to one vulnerabilities entry by sharing the cve_id ￼. The unique index on CVE prevents duplicates, and the ingestion process should check for existing CVEs. If a vulnerability has no CVE, it will rely on the external_source+external_id uniqueness to avoid duplicates from the same source. Cross-vendor deduplication for non-CVE issues is more complex – it might require manual mapping or pattern matching (e.g., matching titles or CPEs). In such cases, the schema can be extended by a vulnerability alias mapping (not explicitly defined here) or by leveraging the field_mapping to direct multiple sources to the same internal vulnerability record. For now, each new scanner finding without CVE creates a new vulnerability record (ensuring data is ingested), and analysts can later merge or mark duplicates if identified.

findings Table
```sql
CREATE TABLE findings (
    finding_id SERIAL PRIMARY KEY,
    asset_id INTEGER NOT NULL REFERENCES assets(asset_id) ON DELETE CASCADE,
    vulnerability_id INTEGER NOT NULL REFERENCES vulnerabilities(vulnerability_id) ON DELETE CASCADE,
    integration_id INTEGER NOT NULL REFERENCES scanner_integration(integration_id) ON DELETE CASCADE,
    first_seen TIMESTAMP,
    last_seen TIMESTAMP,
    status VARCHAR(20),      -- e.g., 'open', 'fixed', 'risk_accepted'
    severity_level SMALLINT, -- normalized severity at finding level
    port VARCHAR(10),        -- network port if applicable (e.g., '443')
    protocol VARCHAR(10),    -- protocol if applicable (e.g., 'TCP')
    details JSONB,
    UNIQUE(asset_id, vulnerability_id, integration_id)
);
```

Description: Represents an instance of a vulnerability on a specific asset, as reported by a particular integration. This is essentially a “vulnerability finding” or “vulnerability occurrence”. Each row ties together an asset, a vulnerability definition, and the scanner that found it, along with context like timestamps, status, and details.
	•	asset_id: Foreign key reference to the affected asset in the assets table. This links the finding to the normalized asset record.
	•	vulnerability_id: Foreign key to the vulnerabilities table, linking to the normalized vulnerability definition (CVE or issue). This structure ensures that all findings of the same underlying issue reference a single vulnerability record (for deduplication of vulnerability info).
	•	integration_id: Foreign key to scanner_integration, indicating which scanner or source reported this finding. This is important because the same asset and vulnerability might be reported by multiple integrations. The composite unique key (asset_id, vulnerability_id, integration_id) ensures no duplicate entry for the same asset/vuln from the same source. If two scanners report the same CVE on the same asset, this will result in two findings records (same asset & vuln, different integration_id) – which allows tracking source-specific details (like different discovery times or statuses), while still tying to one asset and vuln.
	•	first_seen / last_seen: Timestamps for when the vulnerability was first observed on this asset and when it was last confirmed. These can be updated on each import. For example, if a Nessus scan in January finds a vulnerability, first_seen=Jan date; subsequent scans in March still see it, update last_seen=March. If another integration (say Qualys) also reports it, that would be a separate record (different integration) with its own timestamps.
	•	status: The current status of the finding, such as ‘open’ (unresolved/new), ‘fixed’ (no longer detected), ‘risk_accepted’, etc. Some scanners provide status information or a notion of whether the finding is active. For example, Microsoft Defender VM (TVM) might mark a vulnerability as remediated or not applicable when it’s no longer present; Qualys might have a status if a QID is fixed. The platform can update this field accordingly during ingestion (e.g., if a scanner API explicitly flags old findings as fixed).
	•	severity_level: The normalized severity of this finding, stored for quick access. In many cases this will mirror the severity_level in the related vulnerabilities entry (since the vulnerability’s severity is inherent). However, we include it here for flexibility because some platforms or contexts might adjust severity per asset (for example, if a vulnerability is exploited or if a particular asset context increases risk). Typically, this is set via the severity_mapping as data is ingested (using the scanner’s reported severity for that finding).
	•	port / protocol: If the finding is tied to a network service or port, these fields store that information. Network scanners like Nessus, Tenable, Qualys provide port and protocol for many findings (e.g., a vulnerability on port 443/TCP). For host-based findings (like agent-based scanners or configuration issues), these may be NULL. Including these fields supports use cases like filtering vulnerabilities by open port or correlating with service inventories.
	•	details (JSONB): A catch-all for any scanner-specific or extra details about the finding on that asset. This is crucial for storing evidence or context that doesn’t fit into fixed columns. For example, Nessus provides a detailed “plugin output” for each finding (evidence of the vulnerability on that host); this can be stored as JSON in details (potentially with keys like "plugin_output"). Qualys might include info like the output of a vulnerability test, or a list of IPs if the vulnerability spans multiple IPs on the host (for multi-homed devices). CrowdStrike might include a path to a vulnerable software or a registry key. Microsoft Defender VM might include an ID of the software or a link to a Knowledge Base article. Rather than adding separate columns for each such piece of data, the JSON details field can store all these in a structured way. We could also store the raw JSON response from an API (or a reference to it) here for audit purposes.

Normalization & Relationships: The separation of findings from assets and vulnerabilities is a normalization step that avoids data redundancy – asset details are stored once, vulnerability details once, and findings join them. This table also aids deduplication: if the same vulnerability is reported again for the same asset by the same integration, the unique constraint prevents a new duplicate record (the ingestion logic can update last_seen instead). If different integrations report it, they are kept separate here but still linked to the same asset/vuln, allowing the platform to consolidate them in views (so risk is not double-counted) while also preserving source-specific info ￼. Foreign keys with ON DELETE CASCADE ensure data integrity – e.g., deleting an asset will remove its findings, removing an integration can clean up associated findings and mappings, etc.

field_mapping Table
```sql
CREATE TABLE field_mapping (
    mapping_id SERIAL PRIMARY KEY,
    integration_id INTEGER NOT NULL REFERENCES scanner_integration(integration_id) ON DELETE CASCADE,
    source_field VARCHAR(100) NOT NULL,
    target_model VARCHAR(50) NOT NULL,   -- e.g., 'assets', 'vulnerabilities', 'findings'
    target_field VARCHAR(50) NOT NULL,   -- e.g., 'hostname', 'cve_id', 'port'
    transformation VARCHAR(100),
    UNIQUE (integration_id, source_field, target_model, target_field)
);
```

Description: Defines how fields from an external scanner’s data map to fields in our internal data model. This acts as a translation layer so that we can ingest data from various formats (CSV, XML, API JSON) without hardcoding field names. Each mapping row specifies that a given source_field (as named in the raw scanner output) should be stored in a specific target_model.target_field in our database. An optional transformation can indicate any special handling (e.g., date format conversion, splitting strings, or using a lookup like severity mapping).
	•	integration_id: References which integration this mapping applies to. This makes the mapping specific to a scanner, since different scanners use different field names for similar data. For example, Nessus files have a field called “Risk Factor” whereas Qualys might call it “Severity” (with numeric levels); their mappings would be defined separately.
	•	source_field: The exact name (or key) of the field from the source data. This might be a JSON key from an API, an XML tag, or a CSV column name. For instance: “Plugin ID” (Nessus XML tag for plugin identifier), “CVE ID” (if a scanner exports that), “IP Address”, “DnsName”, “Severity”, “Vulnerability ID”, etc., as they appear in that integration’s export.
	•	target_model: The internal table/model that this field maps to. Typically one of 'assets', 'vulnerabilities', or 'findings' for our core schema. This clarifies where the data will end up. For example, an IP address would map to the assets model, while a CVE maps to vulnerabilities, and a specific open port goes to findings.
	•	target_field: The specific column in the target model table where the data will be placed. This should correspond to one of the column names defined in those tables. For example, source_field="Host Name" might map to target_model="assets" and target_field="hostname". Or source_field="QID" (Qualys ID) maps to target_model="vulnerabilities" target_field="external_id".
	•	transformation: Optional instructions or keys for how to transform the source data before storing. This can be used if the source data needs conversion. Examples: 'to_lowercase' to normalize case, 'parse_date:%Y-%m-%d' to parse a date string, or 'severity_map' to indicate that the source field’s value should be looked up in the severity_mapping table to produce an internal severity. For instance, a Nessus source field “Risk Factor” might have values like “High”, and using a transformation (or simply via code) we convert that to a numeric severity_level using our mapping. This field can also indicate concatenation or other custom logic if needed (though complex logic might be handled in code rather than in the table).

Unique Constraint: The unique index on (integration_id, source_field, target_model, target_field) ensures that we don’t accidentally define two conflicting mappings for the same source field. In practice, a source field from a given integration should map to only one destination. (Usually, the combination of integration and source_field is enough to be unique; including the target ensures we don’t list the same mapping twice in different ways.)

Extensibility: This table makes the ingestion pipeline configurable. To support a new field from a scanner, one can add a mapping row rather than altering code or the schema. For example, if a new version of Nessus adds a field “CVSS3_Temporal_Score” in the export, we could decide to map it to an extra JSON key or a new column if added; we’d insert a mapping like (Nessus integration id, 'CVSS3_Temporal_Score', 'vulnerabilities', 'extra', NULL) or similar (storing under extra JSON). If an integration has fields that we initially don’t map, they can be added later via this table, ensuring the system is adaptable to changes in vendor data. It also allows multiple integrations to feed into the same internal fields via different source_field names.

Mapping Examples: (Detailed examples are provided in the next section.) A Nessus .nessus XML file might have a <ReportHost> section with a <HostName> tag – the mapping would route “HostName” -> assets.hostname. Similarly, Nessus <ReportItem> entries have <pluginID> which we map to vulnerabilities.external_id, and <risk_factor> (values like “Critical/High”) which we map via a transformation to our severity. Qualys API might return JSON with "QID" -> vulnerabilities.external_id, "severity" (1–5) -> vulnerabilities.severity_level (via mapping or direct if we treat it as numeric). Microsoft Defender might have a field for the vulnerability ID (GUID) -> vulnerabilities.external_id, etc. All such correspondences are configured here.

severity_mapping Table
```sql
CREATE TABLE severity_mapping (
    severity_mapping_id SERIAL PRIMARY KEY,
    integration_id INTEGER NOT NULL REFERENCES scanner_integration(integration_id) ON DELETE CASCADE,
    external_severity VARCHAR(50) NOT NULL,
    internal_severity_level SMALLINT NOT NULL,
    internal_severity_label VARCHAR(20) NOT NULL,
    UNIQUE(integration_id, external_severity)
);
```

Description: Provides a translation from an external scanner’s severity rating scheme to a normalized internal severity scale. Different scanners use various scales – some use qualitative labels (“Low/Medium/High/Critical”), some use numeric levels (e.g., 1–5, or CVSS scores), and others might use custom risk scores. This table allows each integration to define how its severity values map to the platform’s standard severity representation (both as a numeric level and a label).
	•	integration_id: Reference to the scanner integration. The mapping is defined per integration because a given term might mean different things in different contexts, and not all scanners share the same labels. For example, “Critical” exists in many, but some might use “Urgent” or numeric values instead. By scoping to integration, we can even handle cases where two scanners use the same word but we want to map them differently (though likely “High” means roughly the same risk level universally).
	•	external_severity: The severity value as provided by the scanner. This could be a word (e.g., “Critical”, “High”, “Medium”, “Low”, “Informational”), a number as string (e.g., “5” or “3”), or any rating the scanner uses (for instance, some APIs might return “Critical” or “CRITICAL” – we’d use the exact form or a known canonical form). We store it as text, since even numeric levels often come as strings from APIs; if needed, we could also have a numeric field, but text covers all cases uniformly.
	•	internal_severity_level: A numeric representation of severity in our internal model. We might choose a 0–10 scale (where 10 is most severe) to align with CVSS or a 1–5 scale corresponding to categories. Numeric values allow easier sorting and threshold filtering. In this design, for example, we could use: 10 = Critical, 8 = High, 5 = Medium, 3 = Low, 0 = Informational (as an example mapping). The actual values can be adjusted as needed, but the idea is to normalize different scales onto one quantitative scale.
	•	internal_severity_label: A normalized label for the severity category, to use consistently in the UI or reports (e.g., use standard categories like “Critical/High/Medium/Low/Informational”). This likely aligns with the numeric level (e.g., level 10 -> label “Critical”). Storing the label is somewhat redundant with the level, but it avoids having to join or translate the number back to text when presenting data. It also allows flexibility if we ever had multiple label schemes. Typically, the platform would define a fixed set of internal labels (perhaps aligned with CVSS risk rating names or custom tiers).

Unique Constraint: Each (integration_id, external_severity) pair must be unique, meaning you define at most one mapping for each severity value of a given integration. This prevents conflicting mappings (you wouldn’t want to map “High” to two different levels for the same integration, for instance).

Integration Scope: The mapping is per integration, but note that if multiple integrations coincidentally use the exact same terms and the same intended meaning, you would still insert separate rows for each integration to satisfy the constraint (they can map to the same internal level/label). This explicit per-integration mapping is useful especially when scales differ – e.g., Qualys severity “5” might be labeled “Critical” and map to level 10, whereas Nessus “Critical” maps to 10 as well; a scanner like CrowdStrike might output a CVSS score directly (in which case the mapping might define ranges or not be used at all if we take the score directly).

Usage: During ingestion, whenever a scanner’s finding is processed, its raw severity is looked up in this table to assign the standardized severity_level and severity_label. For example, if Nessus reports a finding with Risk Factor = "High", the integration (Nessus) and value “High” will be found here and translated to (say) internal level 8, label “High”. If Qualys API returns "severity": 4, the Qualys integration’s mapping might translate 4 -> level 8, label “High” as well. This ensures that after ingestion, a combined view of all findings can sort or filter by severity without mixing apples and oranges.

Example Field Mapping Entries (Nessus XML)

To illustrate, here are example field_mapping entries for a Nessus file integration (which imports .nessus XML reports). In a Nessus report, hosts and vulnerabilities are described with specific fields that we map to our schema:

```sql
-- Assume integration_id = 1 corresponds to "Nessus File"
INSERT INTO field_mapping (integration_id, source_field, target_model, target_field, transformation) VALUES
(1, 'HostName',    'assets',         'hostname',       NULL),
(1, 'host-ip',     'assets',         'ip_address',     NULL),
(1, 'operating-system', 'assets',    'operating_system', NULL),
(1, 'MAC-address', 'assets',         'mac_address',    NULL),
(1, 'pluginID',    'vulnerabilities','external_id',    NULL),
(1, 'pluginName',  'vulnerabilities','title',          NULL),
(1, 'cve',         'vulnerabilities','cve_id',         NULL),
(1, 'cvss_base_score', 'vulnerabilities','cvss_score', NULL),
(1, 'risk_factor','vulnerabilities','severity_level', 'severity_map'),  -- uses severity_mapping
(1, 'risk_factor','vulnerabilities','severity_label', 'severity_map'),
(1, 'description','vulnerabilities','description',    NULL),
(1, 'solution',   'vulnerabilities','fix_info',       NULL),
(1, 'plugin_output','findings',     'details',        NULL),
(1, 'port',       'findings',       'port',           NULL),
(1, 'protocol',   'findings',       'protocol',       NULL);
```

Explanation: These mappings cover key fields from Nessus XML:
	•	Host properties like HostName, host-ip, operating-system, MAC-address map into the assets table (hostname, ip_address, etc.). Nessus XML uses tags within <HostProperties> for these (for example, <tag name="host-ip">10.0.0.1</tag>). This mapping ensures when parsing the file, those values end up populating the asset record.
	•	Vulnerability fields from each Nessus <ReportItem>:
	•	pluginID (the Nessus plugin identifier for the vulnerability) maps to vulnerabilities.external_id. We would also set external_source = 'Nessus' for that vulnerability (the code handling ingestion might know the integration context and fill external_source accordingly). This allows us to uniquely identify Nessus findings and correlate if needed ￼.
	•	pluginName (a short title of the vuln) maps to our title.
	•	cve maps to cve_id (Nessus often includes one or multiple CVEs; if multiple, the parser might insert the first or all — for simplicity, we handle it as one field here. Multiple CVEs could be concatenated or the primary one chosen).
	•	cvss_base_score maps to cvss_score (Nessus provides the base score for many findings).
	•	risk_factor (Nessus uses values like “Critical/High/Medium/Low/None”) is mapped to both severity_level and severity_label in vulnerabilities. We use a special transformation 'severity_map' to indicate this should use severity_mapping. Essentially, when the parser sees risk_factor = "High", it will look up in severity_mapping for Nessus and find the internal level (e.g., 8) and label (“High”). Those get populated. We list it twice to fill both fields (alternatively, the ingestion logic could fill both from one mapping).
	•	description and solution from Nessus map to our description and fix_info respectively, enriching the vulnerability record with detailed info and remediation steps.
	•	Finding-specific fields:
	•	plugin_output (Nessus provides this as evidence or output of the check on that host) is mapped to findings.details. Since details is JSONB, the ingestion code might insert the whole plugin output text as a value in the JSON (e.g., {"plugin_output": "…actual output text…"}) or store multiple pieces of info under different keys if needed. Here we treat it generally – the transformation could also split parts if structured (Nessus plugin output is usually unstructured text, so likely just stored as a blob of text in JSON).
	•	port and protocol map directly to the findings table columns of the same name. These give context on where the vulnerability is present (especially for network vulnerabilities). If Nessus lists port=443 and protocol=tcp, our record will reflect that. If a finding has no port (e.g., a configuration issue not tied to a port), those fields would be null.

Similar mappings would be created for other integrations. For example, a Qualys integration’s field_mapping might include entries like: ('QID' -> vulnerabilities.external_id), ('IP' -> assets.ip_address), ('DNS' -> assets.hostname), ('OS' -> assets.operating_system), ('Vulnerability' (title) -> vulnerabilities.title), ('Threat' (description) -> vulnerabilities.description), ('Solution' -> vulnerabilities.fix_info), ('Severity' -> vulnerabilities.severity_level/label via mapping), etc. The mapping table thus acts as a central definition of how to parse and store fields from each source.

Example Severity Mapping Entries

Using Nessus as an example integration, we define how Nessus risk ratings map to internal severity. Nessus categorizes findings as Critical, High, Medium, Low, Informational (Informational is sometimes labeled as None or Info in older versions). We can map these to an internal 0–10 scale and matching labels:

```sql
-- Assume integration_id = 1 is "Nessus File"
INSERT INTO severity_mapping (integration_id, external_severity, internal_severity_level, internal_severity_label) VALUES
(1, 'Critical',     10, 'Critical'),
(1, 'High',         8,  'High'),
(1, 'Medium',       5,  'Medium'),
(1, 'Low',          3,  'Low'),
(1, 'Informational',0,  'Informational');
```

In this example, we assign Critical = 10, High = 8, Medium = 5, Low = 3, Informational = 0. The numeric values are chosen to reflect relative risk and to roughly align with common scoring (for instance, 10 could correspond to CVSS 9.0–10 or a top-tier risk, 8 for high, etc.). The labels are the same as the external in this case, but normalized to our standard set.

For another integration like Qualys, severity levels 1–5 could be mapped as: 5→10 (Critical), 4→8 (High), 3→5 (Medium), 2→3 (Low), 1→1 (Low or Informational depending on interpretation). For Microsoft Defender VM (TVM), which might output severity as “Critical/High/Medium/Low”, we’d have a similar mapping to internal values. CrowdStrike might provide severity in terms of CVSS or a criticality tier; if they provide an actual CVSS score, we might not need a mapping (we could directly use the score as cvss_score and derive severity from that). In that case, we might still have a mapping if CrowdStrike defines something like “Critical” for certain ranges, or we simply normalize their provided score to the nearest category.

Each integration’s severity_mapping can thus be tuned. The platform could decide to use a consistent internal scheme (as above) and all mappings target that scheme ￼. The unique constraint prevents adding, say, two different mappings for “High” under the same integration. If an integration’s external severity values change (imagine a scanner adds “Very High”), we can insert a new row accordingly.

Note: The severity_mapping is typically used in conjunction with the field_mapping. For instance, the field_mapping for Qualys might have source_field='Severity' mapping to vulnerabilities.severity_level with a transformation indicating to apply the mapping (since Qualys “Severity: 4” should translate via the table). Similarly, if an integration gives a numeric risk score out of 100 (hypothetical), we could either map ranges of that into our levels or store it directly and derive level via a formula. This table primarily handles categorical mappings.

Design Rationale and Recommendations

Extensibility: The schema is designed to accommodate new scanners and data changes with minimal effort. Adding a new integration (say, a new cloud vulnerability scanner) involves inserting a row in scanner_integration and populating field_mapping and severity_mapping for it. The core tables (assets, vulnerabilities, findings) do not need to change for each new data source – they capture the superset of information. Vendor-specific fields are either mapped to existing columns or stored in JSONB extras. This decoupling follows an open schema approach where new data can flow in without schema migrations. The mapping tables act as configuration, so that differences in field names or rating schemes between, for example, Tenable and CrowdStrike, are resolved through data configuration rather than code changes. This yields a more maintainable system when integrating dozens of tools (Vulcan Cyber’s platform indeed supports a broad array of ~100 integrations, underscoring the need for such flexibility).

Deduplication Strategy: Data deduplication occurs at multiple levels:
	•	Assets: As noted, asset deduplication is a challenging but crucial part of vulnerability management ￼ ￼. The schema provides a unique index on key fields and encourages consolidating records. When ingesting, the system should attempt to find an existing asset by unique identifiers (like matching IP or hostname, or other attributes). If found, it links findings to that asset instead of creating a new one. Over time, the assets table can accumulate identifiers from various sources in the extra JSON (for example, store multiple IPs or device IDs), aiding correlation. In complex cases (e.g., an asset has different hostnames in different tools), a separate process or admin review might reconcile them. The schema can be extended with an asset_alias table or by storing lists of known aliases in extra if needed. The goal is a “single asset view” so that vulnerabilities aren’t double-counted simply due to naming differences ￼.
	•	Vulnerabilities: Deduplication is primarily via the CVE and the unique keys on vulnerabilities. By using a common identifier (CVE), different scanner findings can map to the same vulnerability entry ￼. For non-CVE issues, each integration’s external_id ensures no duplicates from that integration. Cross-integration dedup for those is harder – potentially approached by comparing titles or CWE IDs. As a recommendation, consider implementing a vulnerability alias mapping or use existing vulnerability intelligence to link scanner-specific IDs to CVEs or global identifiers. The schema’s flexibility allows, for example, adding a cwe_id column or having a mapping table between external_id of one source and external_id of another if known to represent the same issue. In absence of that, such findings remain separate entries but can be merged later if identified.
	•	Findings: The unique (asset, vulnerability, integration) constraint prevents exact duplicate records on repeated imports. If the same scanner reports the same finding again, the system should update the existing record (e.g., bump last_seen). If two scanners report the same asset/vuln, the design deliberately keeps two records (so we know it came from two sources), but since they point to one vulnerability and one asset, an aggregated view can count it once. This approach balances data fidelity with deduplicated views – one can always de-duplicate at query time by grouping on asset+vuln, but still see source-specific metadata if needed.

Normalization: The schema enforces normalization both in the database design sense (avoiding data repetition) and in data standardization:
	•	The separation into assets, vulnerabilities, and findings is a classic normalization (3NF) approach: asset attributes are stored once, vulnerability attributes once, and the join (finding) stores only relationships and context. This reduces inconsistent data – e.g., if a vulnerability’s description needs updating, it’s changed in one place rather than every finding row.
	•	The use of severity_mapping and standardized severity fields ensures that, after ingestion, severity is comparable across all findings. Without this, one tool’s “High” might not align with another’s “High”. By mapping to a common scale, we normalize risk ratings ￼. We also normalized data types (e.g., using INET for IP ensures a consistent format, and possibly automatic normalization like storing IPv4 in a canonical form). Dates, if parsed, would be stored in timestamp fields, etc.
	•	Other normalization in data: CVE IDs provide a normalized key for vulnerabilities; hostnames/IPs provide a way to normalize asset identity (though not perfect). We recommended using authoritative data (like an asset inventory integration) to further normalize asset naming across sources if available.

Foreign Key Usage: Foreign keys link the tables and maintain referential integrity. For example, each finding must refer to a valid asset and vulnerability. The ON DELETE CASCADE is set on these relationships to aid data lifecycle management (e.g., if an asset is removed or merged, its findings can be cleaned up or reassigned properly). However, typically one would not want to delete assets or vulnerabilities outright if they have findings; instead, one might mark them inactive. The foreign keys mainly prevent orphan records. Foreign key to scanner_integration in findings and mapping tables ensures that if an integration is removed or disabled, related config and data can be handled (though usually you’d likely soft-delete or archive rather than hard delete an integration). Notably, we do not include a foreign key to integration in the vulnerabilities or assets tables – this is by design, since those are meant to be global objects not owned by a single integration. A vulnerability can be reported by many integrations, and an asset can be scanned by multiple tools. Thus, integration attribution is handled at the finding level (and via mapping for fields). This avoids tying the lifecycle of a vulnerability or asset to any one integration. If needed, you can create junction tables (e.g., asset_source or vulnerability_source) to list all integrations that have seen a given asset or vulnerability, but the core model keeps them general.

Unstructured Data (JSONB) Recommendations: We include JSONB columns (assets.extra, vulnerabilities.extra, findings.details) to store unstructured or vendor-specific data. This is a deliberate choice to make the schema extensible without needing frequent alterations. When ingesting data from a new scanner, there are often fields that don’t neatly fit the existing columns. For example:
	•	Vuln scanners might have plugin-specific metadata, exploit availability info, or scan IDs.
	•	Cloud vulnerability findings might include cloud metadata (instance type, tags).
	•	Some tools provide rich context (like configuration details for misconfiguration findings).

Instead of introducing new columns for each such field (which would lead to a very wide, sparse table and constant migrations), JSONB allows storing them in a structured way. We recommend using a consistent key naming in JSON for known common concepts (e.g., "tags" for asset tags, "vuln_reference" for any external reference URLs, etc.) so that even within JSON, data from different integrations can converge if they represent the same concept. Data that is truly unique to one integration can just live under a namespaced key (e.g., "qualys_asset_uuid" or "nessus_plugin_family").

PostgreSQL’s JSONB is efficient for storing and querying semi-structured data, and you can even index specific keys if needed for performance. For example, if we frequently query the details->>'plugin_output', we could index that. But generally, JSONB is meant for data that is accessed opportunistically (for troubleshooting or full-detail views) rather than in every query. The main workflow will rely on the mapped, normalized columns for filtering and joining, while the JSONB provides depth when needed (e.g., showing full scanner output to an analyst investigating a finding).

Performance Considerations: Although not explicitly asked, it’s worth noting that this schema, while normalized, is optimized for the typical queries in a vulnerability management context. Common queries might include: “List all open findings for each asset with severity High or above” – this can be done by joining assets->findings->vulnerabilities and filtering on severity_level >= X. Because severity is stored in both findings (for quick filter) and vulnerabilities, we can choose how to query (if severity is truly intrinsic to the vuln, one could just use vulnerability.severity; if per-finding context matters, use findings.severity). Proper indexes (e.g., on findings(severity_level), vulnerabilities(cve_id), etc.) would be created to speed up lookups. The unique constraints and primary keys also implicitly create indexes that help maintain data integrity and query performance on those keys.

Conclusion: The above schema and mapping logic provide a robust foundation for ingesting and consolidating vulnerabilities and assets from multiple scanners. By leveraging mapping tables and JSON flexibility, we ensure that the data model can support all integrations listed in Vulcan Cyber’s public documentation (Tenable.io, Nessus, Qualys, CrowdStrike, Microsoft Defender VM, and many more) without structural changes. This design handles the inherent variability in source data while preserving the ability to correlate and prioritize vulnerabilities across an organization’s entire technology stack. Using this approach, the platform can “pull and ingest assets and vulnerability data” from diverse tools and then “correlate, consolidate, and contextualize” it ￼ – ultimately providing a single pane of glass for vulnerability risk management.